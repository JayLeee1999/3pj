from dotenv import load_dotenv
import os
import json
import glob
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pinecone import Pinecone, ServerlessSpec

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
OPENAI_EMBEDDING_MODEL = os.getenv("OPENAI_EMBEDDING_MODEL")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")
PINECONE_NAMESPACE = os.getenv("PINECONE_NAMESPACE")


def load_latest_current_issues():
    """ìµœì‹  í˜„ì¬ì´ìŠˆ JSON íŒŒì¼ì„ ìë™ìœ¼ë¡œ ë¡œë“œ"""
    json_files = glob.glob("data2/*_BigKinds_current_issues.json")
    if not json_files:
        raise FileNotFoundError("í˜„ì¬ì´ìŠˆ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    latest_file = max(json_files, key=os.path.getctime)
    print(f"ğŸ“‚ ë¡œë“œí•˜ëŠ” íŒŒì¼: {latest_file}")
    
    with open(latest_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    issues = data["issues"]
    docs = [
        Document(
            page_content=f"{item['ì œëª©']}\n{item['ë‚´ìš©']}",
            metadata={"ì´ìŠˆë²ˆí˜¸": item["ì´ìŠˆë²ˆí˜¸"], "ì œëª©": item["ì œëª©"]}
        )
        for item in issues
    ]
    
    return docs


def main():
    # ë¬¸ì„œ ë¡œë“œ
    docs = load_latest_current_issues()
    
    # embedding ëª¨ë¸ ê°ì²´ ìƒì„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    pc = Pinecone(api_key=PINECONE_API_KEY)
    
    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=450, 
        chunk_overlap=45,
        length_function=len,  # ë¬¸ììˆ˜   
        separators=["\n\n", "\n", " ", ""]
    )
    
    # ë¬¸ì„œë¥¼ ë¶„í• 
    chunks = text_splitter.split_documents(docs)
    
    return chunks, embeddings, pc


if __name__ == "__main__":
    chunks, embeddings, pc = main()